
#### general settings

name: InvDN_ResUnit_x4
use_tb_logger: true
# model: InvDN
model: HCFlow_Rescaling
scale: 4
gpu_ids: [0]
distortion: sr

#### datasets

datasets:
  train:
    name: SIDD
    mode: GTLQnpy
    dataroot_GT: '/apdcephfs/private_jacorbzhu/InvDN/codes/SIDD_Medium_Srgb/Data_Train2/GT' # path to training Clean images
    dataroot_Noisy: '/apdcephfs/private_jacorbzhu/InvDN/codes/SIDD_Medium_Srgb/Data_Train2/Noisy' # path to training Noisy images
    dataroot_LQ: ~ # path to training reference LR images, not necessary, if not provided, LR images will be generated in dataloader

    use_shuffle: true
    n_workers: 0  # per GPU
    batch_size: 16
    GT_size: 100
    use_flip: true
    use_rot: true
    color: RGB

  val:
    name: SIDD_val
    mode: GTLQnpy
    dataroot_GT: '/apdcephfs/private_jacorbzhu/InvDN/codes/sidd2/groundtruth' # path to validation Clean images
    dataroot_Noisy: '/apdcephfs/private_jacorbzhu/InvDN/codes/sidd2/input' # path to validation Noisy images
    dataroot_LQ: ~ # path to validation reference LR images, not necessary, if not provided, LR images will be generated in dataloader


#### network structures

network_G:
  which_model_G: HCFlowNet_Rescaling
  subnet_type: Resnet
  in_nc: 3
  out_nc: 3
  act_norm_start_step: 100
  block_num: [8, 8]
  scale: 4
  init: xavier

  flowDownsampler:
    K: 14
    L: 2
    squeeze: haar # better than squeeze2d
    flow_permutation: none # bettter than invconv
    flow_coupling: Affine3shift # better than affine
    nn_module: DenseBlock # better than FCN
    hidden_channels: 32
    cond_channels: ~
    splitOff:
      enable: true
      after_flowstep: [6, 6]
      flow_permutation: invconv
      flow_coupling: Affine
      stage1: True
      feature_extractor: RRDB
      nn_module: FCN
      nn_module_last: Conv2dZeros
      hidden_channels: 64
      RRDB_nb: [2,1]
      RRDB_nf: 64
      RRDB_gc: 16



#### path
#'/data/yongpei/data/InvDN/experiments/InvDN_ResUnit_x4/models/168_G.pth'
path:
  #experiments_root: '/apdcephfs/private_jacorbzhu/InvDN/experiments/InvDN_ResUnit'
  pretrain_model_G: 
  #'/apdcephfs/private_jacorbzhu/InvDN/experiments13/InvDN_ResUnit_x4/models/219500_G.pth'
  strict_load: true
  resume_state: ~
  #'/apdcephfs/private_jacorbzhu/InvDN/experiments13/InvDN_ResUnit_x4/training_state/219500.state'
  #'/data/yongpei/data/InvDN/experiments/InvDN_ResUnit_x4/training_state/168.state'


#### training settings: learning rate scheme, loss

train:
  two_stage_opt: True
  lr_G: !!float 2e-4
  max_grad_clip: 5
  max_grad_norm: 100
  beta1: 0.9
  beta2: 0.999
  niter: 600000
  warmup_iter: -1  # no warm up

  lr_scheme: MultiStepLR
  lr_steps: [100000, 200000, 300000, 400000, 500000]
  lr_gamma: 0.5
  restarts: ~
  restart_weights: ~
  eta_min: !!float 1e-8

  weight_z: !!float 1e-5

  pixel_criterion_lr: l2
  pixel_weight_lr: !!float 5e-2

  pixel_criterion_forw: l2
  pixel_criterion_back: l1

  manual_seed: 10

  val_freq: !!float 200

  lambda_fit_forw: 16.
  lambda_rec_back: 1
  lambda_ce_forw: 1
  weight_decay_G: !!float 1e-8
  gradient_clipping: 10
  eps_std_reverse: 1.0
  pixel_criterion_hr: l1
  pixel_weight_hr: 1.0
  lambda_ce_forw: 1.0

  # perceptual loss
  feature_criterion: l1
  feature_weight: 0
  # gan loss
  gan_type: gan  # gan | lsgan | wgangp | ragan (patchgan uses lsgan)
  gan_weight: 0

  lr_D: 0
  beta1_D: 0.9
  beta2_D: 0.99
  D_update_ratio: 1
  D_init_iters: 1500

#### validation settings
val:
  heats: [0.0, 1.0]
  n_sample: 3


#### logger

logger:
  print_freq: 200
  save_checkpoint_freq: !!float 1e2
